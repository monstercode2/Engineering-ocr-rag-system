"""
InterVL FastAPIæœåŠ¡

æä¾›InterVLå¤šæ¨¡æ€å¤§æ¨¡å‹çš„OCRæœåŠ¡
åŸºäºå®˜æ–¹InternVL3-8Bæ¨¡å‹ï¼Œé€šè¿‡FastAPIå¯¹å¤–æä¾›RESTfulæ¥å£
"""

import os
import io
import json
import torch
import asyncio
import logging
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from transformers import AutoTokenizer, AutoModel
from PIL import Image
import uvicorn

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="InterVL OCR Service",
    description="åŸºäºInternVL3-8Bçš„å·¥ç¨‹æ–‡æ¡£OCRè§£ææœåŠ¡",
    version="1.0.0"
)

# CORSè®¾ç½® - å…è®¸Flaskå‰ç«¯è°ƒç”¨
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5000", "http://127.0.0.1:5000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€é…ç½®
CONFIG = {
    "MODEL_PATH": r"E:\test\ocrsystem\models\internvl3-8b",
    "DEVICE": "cuda" if torch.cuda.is_available() else "cpu",
    "MAX_FILE_SIZE": 500 * 1024 * 1024,  # 500MB (å¢åŠ æ–‡ä»¶å¤§å°é™åˆ¶)
    "SUPPORTED_FORMATS": [".jpg", ".jpeg", ".png", ".pdf", ".bmp", ".tiff"],
    "DEFAULT_PROMPT": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„æŠ€æœ¯å†…å®¹ï¼ŒåŒ…æ‹¬å›¾è¡¨ã€è¡¨æ ¼ã€æ–‡å­—å’ŒæŠ€æœ¯å‚æ•°ï¼Œå¹¶ä¸”ä¸è¦é—æ¼ä»»ä½•ä¸€ä¸ªå­—æˆ–è€…ä¸€å¤„å†…å®¹ã€‚",
}

def build_transform(input_size):
    """æ„å»ºå›¾ç‰‡é¢„å¤„ç†å˜æ¢"""
    from torchvision import transforms
    transform = transforms.Compose([
        transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        transforms.Resize((input_size, input_size), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
    ])
    return transform

def dynamic_preprocess(image, image_size, use_thumbnail=False, max_num=12):
    """åŠ¨æ€é¢„å¤„ç†å›¾ç‰‡"""
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    # è®¡ç®—ç›®æ ‡å°ºå¯¸
    target_ratios = set(
        (i, j) for n in range(1, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= 1
    )
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

    # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ¯”ä¾‹
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    # è®¡ç®—ç›®æ ‡å®½åº¦å’Œé«˜åº¦
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

    # è°ƒæ•´å›¾ç‰‡å¤§å°å¹¶åˆ†å‰²
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        # åˆ†å‰²å›¾ç‰‡
        split_img = resized_img.crop(box)
        processed_images.append(split_img)

    # å¦‚æœä½¿ç”¨ç¼©ç•¥å›¾ï¼Œæ·»åŠ åŸå›¾çš„ç¼©ç•¥å›¾
    if use_thumbnail and len(processed_images) != 1:
        thumbnail = image.resize((image_size, image_size))
        processed_images.append(thumbnail)

    return processed_images

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    """æ‰¾åˆ°æœ€æ¥è¿‘çš„å®½é«˜æ¯”"""
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def load_image(image, input_size=448, max_num=12):
    """åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡"""
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(img) for img in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values

class InterVLModelManager:
    """InterVLæ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.model_path = Path(CONFIG["MODEL_PATH"])
        self.device = CONFIG["DEVICE"]
        self.is_loaded = False
        
    def load_model(self):
        """åŠ è½½InternVLæ¨¡å‹"""
        try:
            logger.info(f"å¼€å§‹åŠ è½½InterVLæ¨¡å‹: {self.model_path}")
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„
            if not self.model_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            
            # åŠ è½½tokenizer
            logger.info("åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(self.model_path), 
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹
            logger.info("åŠ è½½æ¨¡å‹...")
            if self.device == "cuda":
                self.model = AutoModel.from_pretrained(
                    str(self.model_path),
                    torch_dtype=torch.bfloat16,
                    low_cpu_mem_usage=True,
                    use_flash_attn=True,
                    trust_remote_code=True
                ).eval().cuda()
            else:
                # CPUæ¨¡å¼
                self.model = AutoModel.from_pretrained(
                    str(self.model_path),
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                ).eval()
            
            self.is_loaded = True
            logger.info("âœ… InterVLæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.is_loaded = False
            raise e
    
    def process_image(self, image: Image.Image, prompt: Optional[str] = None) -> Dict[str, Any]:
        """å¤„ç†å›¾ç‰‡ï¼Œè¿”å›OCRç»“æœ"""
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        try:
            start_time = datetime.now()
            
            # ä½¿ç”¨é»˜è®¤æç¤ºè¯æˆ–è‡ªå®šä¹‰æç¤ºè¯
            if prompt is None:
                prompt = CONFIG["DEFAULT_PROMPT"]
            
            # å›¾ç‰‡é¢„å¤„ç†
            pixel_values = load_image(image, max_num=12)
            pixel_values = pixel_values.to(self.device)
            if self.device == "cuda":
                pixel_values = pixel_values.to(torch.bfloat16)
            
            # ç”Ÿæˆé…ç½®
            generation_config = dict(max_new_tokens=1024, do_sample=False)
            
            # è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
            question = f'<image>\n{prompt}'
            
            with torch.no_grad():
                response, history = self.model.chat(
                    self.tokenizer, 
                    pixel_values, 
                    question, 
                    generation_config,
                    history=None, 
                    return_history=True
                )
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # è§£æç»“æ„åŒ–å†…å®¹ï¼ˆç®€å•ç¤ºä¾‹ï¼‰
            structured_content = self._parse_structured_content(response)
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "status": "success",
                "raw_text": response,
                "confidence": 0.95,  # InterVLæ²¡æœ‰ç›´æ¥çš„ç½®ä¿¡åº¦è¾“å‡ºï¼Œè¿™é‡Œè®¾ç½®å›ºå®šå€¼
                "metadata": {
                    "model": "internvl3-8b",
                    "device": self.device,
                    "prompt": prompt,
                    "processing_time": processing_time,
                    "image_patches": pixel_values.shape[0]
                },
                "structured_content": structured_content
            }
            
            logger.info(f"âœ… å›¾ç‰‡å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å›¾ç‰‡å¤„ç†å¤±è´¥: {e}")
            raise e
    
    def _parse_structured_content(self, text: str) -> Dict[str, List]:
        """ç®€å•è§£æç»“æ„åŒ–å†…å®¹"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„ç»“æ„åŒ–è§£æé€»è¾‘
            # ç›®å‰åªæ˜¯ç®€å•çš„å…³é”®è¯åŒ¹é…
            
            tables = []
            diagrams = []
            annotations = []
            specifications = []
            
            # æ£€æµ‹è¡¨æ ¼ç›¸å…³å†…å®¹
            if any(keyword in text.lower() for keyword in ['è¡¨æ ¼', 'æ•°æ®', 'å‚æ•°', 'æŒ‡æ ‡']):
                tables.append({
                    "type": "detected_table",
                    "content": "æ£€æµ‹åˆ°è¡¨æ ¼å†…å®¹",
                    "confidence": 0.8
                })
            
            # æ£€æµ‹å›¾è¡¨ç›¸å…³å†…å®¹
            if any(keyword in text.lower() for keyword in ['å›¾', 'å›¾è¡¨', 'ç¤ºæ„å›¾', 'æµç¨‹']):
                diagrams.append({
                    "type": "detected_diagram",
                    "content": "æ£€æµ‹åˆ°å›¾è¡¨å†…å®¹",
                    "confidence": 0.8
                })
            
            # æ£€æµ‹æŠ€æœ¯è§„æ ¼
            if any(keyword in text.lower() for keyword in ['è§„æ ¼', 'å‚æ•°', 'æŠ€æœ¯æŒ‡æ ‡', 'æ€§èƒ½']):
                specifications.append({
                    "type": "technical_specs",
                    "content": "æ£€æµ‹åˆ°æŠ€æœ¯è§„æ ¼",
                    "confidence": 0.8
                })
            
            return {
                "tables": tables,
                "diagrams": diagrams,
                "annotations": annotations,
                "specifications": specifications
            }
            
        except Exception as e:
            logger.warning(f"ç»“æ„åŒ–å†…å®¹è§£æå¤±è´¥: {e}")
            return {
                "tables": [],
                "diagrams": [],
                "annotations": [],
                "specifications": []
            }

# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = InterVLModelManager()

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    try:
        logger.info("ğŸš€ å¯åŠ¨InterVL OCRæœåŠ¡...")
        model_manager.load_model()
        logger.info("ğŸ‰ æœåŠ¡å¯åŠ¨å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        # å¯ä»¥é€‰æ‹©ç»§ç»­å¯åŠ¨ä½†æ ‡è®°ä¸ºä¸å¯ç”¨çŠ¶æ€

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - æœåŠ¡ä¿¡æ¯"""
    return {
        "service": "InterVL OCR Service",
        "version": "1.0.0",
        "model": "internvl3-8b",
        "status": "ready" if model_manager.is_loaded else "loading",
        "device": CONFIG["DEVICE"],
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy" if model_manager.is_loaded else "unhealthy",
        "service": "InterVL OCR",
        "model": "internvl3-8b",
        "model_loaded": model_manager.is_loaded,
        "device": CONFIG["DEVICE"],
        "gpu_available": torch.cuda.is_available(),
        "timestamp": datetime.now().isoformat()
    }

@app.post("/ocr/process")
async def process_document(
    file: UploadFile = File(...),
    prompt: Optional[str] = None
):
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£/å›¾ç‰‡ï¼Œè¿›è¡ŒOCRè¯†åˆ«
    
    å‚æ•°:
    - file: ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆå›¾ç‰‡æˆ–PDFï¼‰
    - prompt: å¯é€‰çš„è‡ªå®šä¹‰æç¤ºè¯
    
    è¿”å›:
    - OCRè¯†åˆ«ç»“æœï¼ŒåŒ…å«æ–‡æœ¬ã€ç»“æ„åŒ–å†…å®¹ç­‰
    """
    try:
        # éªŒè¯æ¨¡å‹çŠ¶æ€
        if not model_manager.is_loaded:
            raise HTTPException(
                status_code=503, 
                detail="æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç¨åé‡è¯•"
            )
        
        # éªŒè¯æ–‡ä»¶å¤§å°
        if file.size and file.size > CONFIG["MAX_FILE_SIZE"]:
            raise HTTPException(
                status_code=413,
                detail=f"æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§æ”¯æŒ {CONFIG['MAX_FILE_SIZE'] // (1024*1024)}MB"
            )
        
        # éªŒè¯æ–‡ä»¶æ ¼å¼
        file_ext = Path(file.filename).suffix.lower()
        if file_ext not in CONFIG["SUPPORTED_FORMATS"]:
            raise HTTPException(
                status_code=400,
                detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ï¼Œæ”¯æŒ: {CONFIG['SUPPORTED_FORMATS']}"
            )
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_content = await file.read()
        
        # è½¬æ¢ä¸ºPILå›¾ç‰‡
        try:
            image = Image.open(io.BytesIO(file_content))
            # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆå¦‚æœä¸æ˜¯çš„è¯ï¼‰
            if image.mode != 'RGB':
                image = image.convert('RGB')
        except Exception as e:
            raise HTTPException(
                status_code=400,
                detail=f"æ— æ³•æ‰“å¼€å›¾ç‰‡æ–‡ä»¶: {e}"
            )
        
        # è®°å½•å¤„ç†å¼€å§‹
        start_time = datetime.now()
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}")
        
        # è°ƒç”¨æ¨¡å‹å¤„ç†
        result = model_manager.process_image(image, prompt)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = (datetime.now() - start_time).total_seconds()
        result["metadata"]["total_processing_time"] = processing_time
        
        # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
        result["file_info"] = {
            "filename": file.filename,
            "size": file.size,
            "format": file_ext,
            "image_size": f"{image.width}x{image.height}"
        }
        
        logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file.filename}, è€—æ—¶: {processing_time:.2f}ç§’")
        
        return JSONResponse(content=result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†å¤±è´¥: {str(e)}")

@app.post("/ocr/batch")
async def batch_process_documents(
    files: List[UploadFile] = File(...),
    prompt: Optional[str] = None
):
    """
    æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£
    """
    try:
        if not model_manager.is_loaded:
            raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
        
        if len(files) > 10:  # é™åˆ¶æ‰¹é‡å¤„ç†æ•°é‡
            raise HTTPException(status_code=400, detail="æ‰¹é‡å¤„ç†æœ€å¤šæ”¯æŒ10ä¸ªæ–‡ä»¶")
        
        results = []
        for file in files:
            try:
                # é‡ç”¨å•æ–‡ä»¶å¤„ç†é€»è¾‘
                file_content = await file.read()
                image = Image.open(io.BytesIO(file_content))
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                result = model_manager.process_image(image, prompt)
                result["file_info"] = {
                    "filename": file.filename,
                    "size": file.size
                }
                results.append(result)
                
            except Exception as e:
                results.append({
                    "status": "error",
                    "filename": file.filename,
                    "error": str(e)
                })
        
        return JSONResponse(content={
            "status": "completed",
            "total_files": len(files),
            "results": results
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")

@app.get("/model/info")
async def get_model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return {
        "model_name": "internvl3-8b",
        "model_path": str(CONFIG["MODEL_PATH"]),
        "device": CONFIG["DEVICE"],
        "is_loaded": model_manager.is_loaded,
        "supported_formats": CONFIG["SUPPORTED_FORMATS"],
        "max_file_size_mb": CONFIG["MAX_FILE_SIZE"] // (1024 * 1024),
        "gpu_available": torch.cuda.is_available(),
        "timestamp": datetime.now().isoformat()
    }

@app.post("/model/reload")
async def reload_model():
    """é‡æ–°åŠ è½½æ¨¡å‹"""
    try:
        logger.info("ğŸ”„ é‡æ–°åŠ è½½æ¨¡å‹...")
        model_manager.load_model()
        return {
            "status": "success",
            "message": "æ¨¡å‹é‡æ–°åŠ è½½æˆåŠŸ",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹é‡æ–°åŠ è½½å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¨¡å‹é‡æ–°åŠ è½½å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    # å¯åŠ¨æœåŠ¡
    logger.info("ğŸš€ å¯åŠ¨InterVL FastAPIæœåŠ¡...")
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=False,  # ç”Ÿäº§ç¯å¢ƒå»ºè®®å…³é—­
        log_level="info"
    ) 